{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnnbeVC7GM9HanUv8C+X2m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u9999yoko/colab/blob/main/anonym_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratoryを便利に使うためのTIPSまとめ\n",
        "https://karaage.hatenadiary.jp/entry/2018/12/17/073000#%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E3%82%A2%E3%83%83%E3%83%97%E3%83%AD%E3%83%BC%E3%83%89%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89"
      ],
      "metadata": {
        "id": "zqEJB3wGKTXN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfQ8ZbJakGg3",
        "outputId": "52ce4fe0-735a-41f6-e569-f4444f1676ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "CPU times: user 106 ms, sys: 15.5 ms, total: 122 ms\n",
            "Wall time: 12.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install pandas regex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ja_ginza --break-system-packages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c836r_no1wYT",
        "outputId": "d56712fd-cfb3-4269-be14-d73f64423dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ja_ginza\n",
            "  Downloading ja_ginza-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from ja_ginza) (3.7.5)\n",
            "Collecting sudachipy<0.7.0,>=0.6.2 (from ja_ginza)\n",
            "  Downloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20210802 (from ja_ginza)\n",
            "  Downloading SudachiDict_core-20240716-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ginza<5.3.0,>=5.2.0 (from ja_ginza)\n",
            "  Downloading ginza-5.2.0-py3-none-any.whl.metadata (448 bytes)\n",
            "Collecting plac>=1.3.3 (from ginza<5.3.0,>=5.2.0->ja_ginza)\n",
            "  Downloading plac-1.4.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ja_ginza) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->ja_ginza) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ja_ginza) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ja_ginza) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ja_ginza) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ja_ginza) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ja_ginza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ja_ginza) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ja_ginza) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->ja_ginza) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->ja_ginza) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja_ginza) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja_ginza) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja_ginza) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ja_ginza) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ja_ginza) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.4.4->ja_ginza) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->ja_ginza) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja_ginza) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja_ginza) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ja_ginza) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ja_ginza) (0.1.2)\n",
            "Downloading ja_ginza-5.2.0-py3-none-any.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ginza-5.2.0-py3-none-any.whl (21 kB)\n",
            "Downloading SudachiDict_core-20240716-py3-none-any.whl (72.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
            "Installing collected packages: sudachipy, plac, sudachidict-core, ginza, ja_ginza\n",
            "Successfully installed ginza-5.2.0 ja_ginza-5.2.0 plac-1.4.3 sudachidict-core-20240716 sudachipy-0.6.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://qiita.com/itsuki3/items/22e7d7a91ec820ab654e"
      ],
      "metadata": {
        "id": "3obHuyoz2Hv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import string\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "2sGh5tM-49bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 市区町村コード一覧（20240101現在の団体.csv）がなかったらアップロードする\n",
        "from google.colab import files\n",
        "org_file = './20240101現在の団体.csv'\n",
        "is_file = os.path.isfile(org_file)\n",
        "if is_file:\n",
        "  print('file exist')\n",
        "else:\n",
        "  uploaded = files.upload()\n",
        "\n",
        "import glob\n",
        "# 特定のパターンに一致するファイルを検索\n",
        "txt_files = glob.glob(\"2*.txt\")\n",
        "print(\"テキストファイルの一覧:\")\n",
        "for file in txt_files:\n",
        "    print(f\"{file}\")\n",
        "\n",
        "file_txt = txt_files[0]\n",
        "print(file_txt)\n",
        "out_file_txt = file_txt.replace(\".txt\", \"_anonym.txt\")\n",
        "print(out_file_txt)\n",
        "\n",
        "# pathlib_path = Path(\"/content/\")\n",
        "# file_list_txt =glob.glob(r'2*.txt', recursive=True)\n",
        "# is_file = file_list_txt[0]\n",
        "# if is_file:\n",
        "#   print('file exist')\n",
        "# else:\n",
        "#   uploaded = files.upload()\n",
        "\n",
        "# # クリーンアップ\n",
        "# for file in glob.glob(\"**/*.txt\", recursive=True):\n",
        "#     os.remove(file)\n",
        "# os.rmdir(\"subdir\")\n",
        "# print(\"\\nテスト用ファイルとディレクトリを削除しました\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-XV0u8JnVMb",
        "outputId": "3c0f554e-cea3-41fd-fe83-1d8f417c322c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file exist\n",
            "テキストファイルの一覧:\n",
            "20240722_1004JATA.txt\n",
            "20240722_1004JATA.txt\n",
            "20240722_1004JATA_anonym.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import secrets\n",
        "import string\n",
        "\n",
        "def generate_secure_random_string(length=12):\n",
        "    \"\"\"Generate a secure random string of specified length.\"\"\"\n",
        "    alphabet = string.ascii_letters + string.digits\n",
        "    return ''.join(secrets.choice(alphabet) for i in range(length))\n",
        "\n",
        "def anonymize_with_pattern(patterns, text):\n",
        "    \"\"\"\n",
        "    Replaces matches of patterns with random strings,\n",
        "    excluding those in brackets with specific time formats.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Protect numbers inside brackets with a temporary placeholder\n",
        "    # (Before replacing brackets with special symbols)\n",
        "    bracketed_numbers = re.findall(r\"(?:\\d+[:.]\\d+|\\d+[hms])\", text) # Changed pattern to match original brackets\n",
        "    # print(\"aaa\", bracketed_numbers)\n",
        "    for i, num in enumerate(bracketed_numbers):\n",
        "        text = text.replace(num, f\"[_PROTECTED_NUMBER_{i}]\", 1)  # Changed placeholder format\n",
        "\n",
        "    # 2. Replace brackets with special symbols to avoid regex conflicts\n",
        "    text = text.replace(\"[\", \"｟\")\n",
        "    text = text.replace(\"]\", \"｠\")\n",
        "    # print(\"sss\" , text)\n",
        "\n",
        "    # 3. Anonymize the text, excluding protected numbers\n",
        "    for pattern in patterns:\n",
        "        # Skip the exclusion pattern (if any)\n",
        "        if pattern.startswith(\"!\"):\n",
        "            continue\n",
        "\n",
        "        # Find all matches of the current pattern\n",
        "        matches = list(re.finditer(pattern, text))\n",
        "\n",
        "        # Iterate through matches in reverse order to avoid index issues\n",
        "        for match in reversed(matches):\n",
        "            # Replace the match with a random string\n",
        "            text = text[:match.start()] + generate_secure_random_string() + text[match.end():]\n",
        "\n",
        "    # 4. Restore the protected numbers\n",
        "    for i, num in enumerate(bracketed_numbers):\n",
        "        text = text.replace(f\"｟_PROTECTED_NUMBER_{i}｠\", num, 1)\n",
        "\n",
        "    # 5. Revert special symbols back to brackets\n",
        "    text = text.replace(\"｟\", \"[\")\n",
        "    text = text.replace(\"｠\", \"]\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "fJf4iNWrMv_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anonymize_PROPN(doc):\n",
        "  \"\"\"Anonymizes a spaCy Doc object by replacing sensitive tokens with anonymized versions.\"\"\"\n",
        "  new_tokens = []\n",
        "  org_tokens = []\n",
        "  # Store the whitespaces of original tokens\n",
        "  whitespaces = [token.whitespace_ for token in doc]\n",
        "  # Anonymize tokens based on their POS tags\n",
        "  i = 0\n",
        "\n",
        "  for token in doc:\n",
        "    i +=1\n",
        "    if i < 8:\n",
        "      continue\n",
        "    print(token.text)\n",
        "    if token.pos_ == \"PROPN\":\n",
        "      if token.tag_ == \"名詞-数詞\":\n",
        "        new_tokens.append(token.text)\n",
        "      elif token.tag_ == \"接尾辞-名詞的-一般\":\n",
        "        new_tokens.append(token.text)\n",
        "      elif token.tag_ == \"感動詞-フィラー\":\n",
        "        new_tokens.append(token.text)\n",
        "      elif token.tag_ == \"名詞-サ変接続\":\n",
        "        new_tokens.append(token.text)\n",
        "      elif token.tag_ == \"名詞-普通名詞-一般\":\n",
        "        new_tokens.append(token.text)\n",
        "      else:\n",
        "        # print(token.text)\n",
        "        # print(generate_secure_random_string())\n",
        "        new_tokens.append(generate_secure_random_string())\n",
        "        org_tokens.append(token.text) # Keep original token\n",
        "    else:\n",
        "      new_tokens.append(token.text)\n",
        "  # print(len(org_tokens))\n",
        "  # Check if org_tokens is not empty before accessing elements\n",
        "  # if new_tokens:\n",
        "    # print(len(new_tokens))\n",
        "  # Create a new Doc with anonymized tokens\n",
        "  new_doc = spacy.tokens.Doc(doc.vocab, words=new_tokens, spaces=whitespaces) # using spaces parameter\n",
        "  return new_doc"
      ],
      "metadata": {
        "id": "InwjFkqftMzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def botu_anonymize_PROPN(doc):\n",
        "  \"\"\"Anonymizes a spaCy Doc object by replacing sensitive tokens with anonymized versions.\"\"\"\n",
        "  new_tokens = []\n",
        "  org_tokens = []\n",
        "  # Store the whitespaces of original tokens\n",
        "  whitespaces = [token.whitespace_ for token in doc]\n",
        "  # Anonymize tokens based on their POS tags\n",
        "  for token in doc:\n",
        "    # print(token.text)\n",
        "    if token.pos_ == \"PROPN\":\n",
        "      if token.tag_ == \"名詞-数詞\":\n",
        "        new_tokens.append(token.text)\n",
        "      elif token.tag_ == \"接尾辞-名詞的-一般\":\n",
        "        new_tokens.append(token.text)\n",
        "      elif token.tag_ == \"感動詞-フィラー\":\n",
        "        new_tokens.append(token.text)\n",
        "      elif token.tag_ == \"名詞-サ変接続\":\n",
        "        new_tokens.append(token.text)\n",
        "      elif token.tag_ == \"名詞-普通名詞-一般\":\n",
        "        new_tokens.append(token.text)\n",
        "      else:\n",
        "        # print(token.text)\n",
        "        # print(generate_secure_random_string())\n",
        "        new_tokens.append(generate_secure_random_string())\n",
        "        org_tokens.append(token.text) # Keep original token\n",
        "    else:\n",
        "      new_tokens.append(token.text)\n",
        "  print(len(org_tokens))\n",
        "  # Create a new Doc with anonymized tokens\n",
        "  new_doc = spacy.tokens.Doc(doc.vocab, words=new_tokens, spaces=whitespaces) # using spaces parameter\n",
        "  return new_doc"
      ],
      "metadata": {
        "id": "GOdH5ommzf14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://qiita.com/CaughC/items/a67a2c81833c\n",
        "# 基本 - 複数文章の同時処理\n",
        "import spacy\n",
        "# !pip install ja-ginza-electra\n",
        "print(spacy.prefer_gpu()) # Trueと表示されれば, GPUが使用されます．\n",
        "if(spacy.prefer_gpu()):\n",
        "  nlp = spacy.load('ja_ginza_electra')\n",
        "else:\n",
        "  nlp = spacy.load('ja_ginza')\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "\n",
        "def delete_begin(text):\n",
        "  pattern = r\"\\d.*[-]+>\"\n",
        "  text = re.sub(pattern, \"\", text)  # マッチした部分を空文字に置換\n",
        "  return text\n",
        "\n",
        "def create_list_from_txt(filetxt):\n",
        "  with open(filetxt, \"r\", encoding=\"utf-8\") as f:\n",
        "    # 空行を削除\n",
        "    lines = [line.strip() for line in f if line.strip()]  # 空行は無視\n",
        "    lines2 = [delete_begin(line) for line in lines]\n",
        "  return lines2\n",
        "\n",
        "list_txt = []\n",
        "list_txt = create_list_from_txt(file_txt)\n",
        "# text を Doc クラスに変換する\n",
        "list_of_docs =nlp.pipe(list_txt)  # custom_nlp_pipe関数を使用\n",
        "# Convert the iterable to a list\n",
        "# # # Convert the iterable to a list\n",
        "anonymized_docs = [anonymize_PROPN(doc) for doc in list_of_docs]  # Convert the iterable to a list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "MThZDaDxx-Fv",
        "outputId": "00281f31-4f1a-4155-f7e5-b4d9004feaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "と\n",
            "か\n",
            "から\n",
            "何\n",
            "か\n",
            "診断\n",
            "さ\n",
            "れ\n",
            "た\n",
            "結果\n",
            "が\n",
            "、\n",
            "そこ\n",
            "で\n",
            "刺さ\n",
            "れ\n",
            "た\n",
            "の\n",
            "だろう\n",
            "と\n",
            "いう\n",
            "結果\n",
            "だ\n",
            "と\n",
            "いう\n",
            "こと\n",
            "です\n",
            "か\n",
            "ね\n",
            "。\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E027] Arguments `words` and `spaces` should be sequences of the same length, or `spaces` should be left default at None. `spaces` should be a sequence of booleans, with True meaning that the word owns a ' ' character following it.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-7dc4d052df71>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Convert the iterable to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# # # Convert the iterable to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0manonymized_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manonymize_PROPN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_docs\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Convert the iterable to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-7dc4d052df71>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Convert the iterable to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# # # Convert the iterable to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0manonymized_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manonymize_PROPN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_docs\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Convert the iterable to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-83-ebf5cb51d077>\u001b[0m in \u001b[0;36manonymize_PROPN\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# print(len(new_tokens))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;31m# Create a new Doc with anonymized tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mnew_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhitespaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# using spaces parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnew_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E027] Arguments `words` and `spaces` should be sequences of the same length, or `spaces` should be left default at None. `spaces` should be a sequence of booleans, with True meaning that the word owns a ' ' character following it."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(anonymized_docs[7:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lY-ZM_5vtER",
        "outputId": "9ec5ad28-c4a7-4956-9c2e-d880bb3ae938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[77.24s] 予約した時にも楽天月末セールと書いてありますし、mknpVoN50hA9さんがお付けになったお値段かなと、お付けになって企画なさったのかなと、, [80.58s] こちらも誤解するような。, [92.58s] とも書いてありますし、予約確認、それから前日の確認、そこでも手配旅行という文字はどこにも見当たらなかったんですね。]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_tag_PROPN(doc):\n",
        "  for token in doc:\n",
        "    if token.pos_ == \"PROPN\":\n",
        "      return token.tag_\n",
        "\n",
        "# Example Code for Rule-Based Reclassification\n",
        "def reclassify_numbers(doc):\n",
        "  for token in doc:\n",
        "    if token.pos_ == \"PROPN\" and token.text.replace(\".\", \"\", 1).isdigit():\n",
        "      # print(token.tag_, token.text)\n",
        "      token.pos_ = \"NUM\"\n",
        "  return doc\n",
        "\n",
        "\"\"\"\n",
        "Text: もとの語\n",
        "Lemma: 語の基本形\n",
        "POS: シンプルな品詞タグ\n",
        "Tag: 詳細な品詞タグ\n",
        "Dep: 統語的依存関係；トークンどうしの関係\n",
        "Shape: 語形；大文字小文字，カンマ／ピリオドの使い方，数字，記号などの分類\n",
        "is alpha: 英字か否か\n",
        "is stop: stop words リストに含まれるか否か；高頻出語か否か\n",
        "\"\"\"\n",
        "def delete_FAKE_PROPN(doc):\n",
        "  for token in doc:\n",
        "    if('月末' in token.text):\n",
        "      print(token.text)\n",
        "    if token.pos_ == \"PROPN\":\n",
        "      if token.tag_ == \"名詞-数詞\":\n",
        "        continue\n",
        "      elif token.tag_ == \"接尾辞-名詞的-一般\":\n",
        "        continue\n",
        "      elif token.tag_ == \"感動詞-フィラー\":\n",
        "        continue\n",
        "      elif token.tag_ == \"名詞-サ変接続\":\n",
        "        continue\n",
        "      elif token.tag_ == \"名詞-普通名詞-一般\":\n",
        "        continue\n",
        "      else:\n",
        "        # if token.text=='None':\n",
        "        # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
        "          # continue\n",
        "        return token.text\n",
        "\n",
        "list_txt = []\n",
        "list_txt = create_list_from_txt(file_txt)\n",
        "# text を Doc クラスに変換する\n",
        "list_of_docs =nlp.pipe(list_txt)  # custom_nlp_pipe関数を使用\n",
        "list_token_pos = []\n",
        "list_PROPN =[]\n",
        "list_token_tag =[]\n",
        "list_PROPN_NUM =[]\n",
        "list_tag_PROPN =[]\n",
        "for doc in list_of_docs:\n",
        "  list_tag_PROPN.extend([select_tag_PROPN(doc) for token in doc])\n",
        "  # Doc クラスは Token クラスのイテレーターになっている\n",
        "  list_token_pos.extend([token.pos_ for token in doc] )\n",
        "  list_PROPN.extend([delete_FAKE_PROPN(doc) for token in doc])\n",
        "  list_token_tag.extend([token.tag_ for token in doc])\n",
        "  list_PROPN_NUM.extend([reclassify_numbers(doc) for token in doc])\n",
        "\n",
        "# Universal POS tags list\n",
        "print(set(list_tag_PROPN))\n",
        "print(set(list_token_pos))\n",
        "print(set(list_PROPN))\n",
        "print(set(list_token_tag))\n",
        "\n",
        "list_tag_PROPN = [item for item in list_tag_PROPN if item is not None]  # Filter out None values from list_PROPN\n",
        "list_PROPN = [item for item in list_PROPN if item is not None]  # Filter out None values from list_PROPN\n",
        "print(list_tag_PROPN)\n",
        "print(list_PROPN)\n",
        "list_set_tag_PROPN = list(set(list_tag_PROPN))\n",
        "[i for i in list_set_tag_PROPN if '名詞' not in i]\n",
        "for prpon in list_set_tag_PROPN:\n",
        "  if '名詞' not in prpon:\n",
        "    print(prpon)\n",
        "\n",
        "anonymized_docs = [anonymize_PROPN(doc) for doc in list_of_docs]  # Convert the iterable to a list\n",
        "print(anonymized_docs[:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XY9UqfBMa2VI",
        "outputId": "8f72a930-ce9c-4168-a75b-a935de14ba79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nText: もとの語\\nLemma: 語の基本形\\nPOS: シンプルな品詞タグ\\nTag: 詳細な品詞タグ\\nDep: 統語的依存関係；トークンどうしの関係\\nShape: 語形；大文字小文字，カンマ／ピリオドの使い方，数字，記号などの分類\\nis alpha: 英字か否か\\nis stop: stop words リストに含まれるか否か；高頻出語か否か\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "月末\n",
            "{'名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-姓', '名詞-固有名詞-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-国', '名詞-普通名詞-一般', '名詞-数詞', '感動詞-フィラー', None}\n",
            "{'PROPN', 'VERB', 'CCONJ', 'SCONJ', 'PRON', 'ADP', 'AUX', 'DET', 'INTJ', 'PUNCT', 'X', 'ADJ', 'ADV', 'NOUN', 'SYM', 'NUM', 'PART'}\n",
            "{'高橋', '時藤', '横山', '楽天トラベル', 'ラスール', '寛', 'ジャパン', '広瀬', '楽天', '栗原', '東京都', '日本', '学研', None}\n",
            "{'感動詞-一般', '助動詞', '補助記号-句点', '助詞-係助詞', '形容詞-非自立可能', '接尾辞-名詞的-副詞可能', '助詞-準体助詞', '名詞-普通名詞-助数詞可能', '補助記号-括弧閉', '名詞-普通名詞-副詞可能', '接尾辞-名詞的-一般', '形容詞-一般', '接尾辞-名詞的-助数詞', '名詞-数詞', '補助記号-読点', '副詞', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-姓', '名詞-固有名詞-地名-国', '接尾辞-形容詞的', '連体詞', '名詞-普通名詞-一般', '形状詞-一般', '補助記号-括弧開', '名詞-固有名詞-人名-一般', '補助記号-一般', '助詞-終助詞', '助詞-接続助詞', '名詞-普通名詞-サ変可能', '名詞-普通名詞-サ変形状詞可能', '接頭辞', '感動詞-フィラー', '名詞-固有名詞-一般', '名詞-普通名詞-形状詞可能', '名詞-固有名詞-地名-一般', '動詞-一般', '接続詞', '助詞-格助詞', '助詞-副助詞', '形状詞-助動詞語幹', '代名詞', '動詞-非自立可能'}\n",
            "['名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-一般', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-一般', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '名詞-固有名詞-人名-名', '感動詞-フィラー', '感動詞-フィラー', '感動詞-フィラー', '感動詞-フィラー', '感動詞-フィラー', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-地名-国', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-固有名詞-人名-一般', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-普通名詞-一般', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-固有名詞-人名-姓', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞', '名詞-数詞']\n",
            "['楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '東京都', '東京都', '東京都', '東京都', '東京都', '東京都', '東京都', '東京都', '東京都', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '学研', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '日本', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '横山', '横山', '横山', '横山', '横山', '横山', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', 'ジャパン', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', 'ラスール', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '楽天', '栗原', '栗原', '栗原', '栗原', '栗原', '栗原', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '高橋', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '寛', '寛', '寛', '寛', '寛', '広瀬', '広瀬', '広瀬', '広瀬', '広瀬', '広瀬', '広瀬', '広瀬', '広瀬', '広瀬', '広瀬', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '時藤', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '楽天トラベル', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山', '横山']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['感動詞-フィラー']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "感動詞-フィラー\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 市区町村データ\n",
        "def create_list_municipalities(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file containing municipality data and returns a list of municipality names.\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file.\n",
        "    Returns:\n",
        "        list: A list of municipality names as strings.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at path: {file_path}\")\n",
        "        return []  # Return an empty list if file not found\n",
        "\n",
        "    # Find the column containing municipality names\n",
        "    municipality_column = next((col for col in df.columns if '市区町村名' in col), None)\n",
        "\n",
        "    if municipality_column is None:\n",
        "        print(\"Error: Column containing '市区町村名' not found in the CSV file.\")\n",
        "        return []  # Return an empty list if column not found\n",
        "\n",
        "    # Extract and clean municipality names\n",
        "    municipalities = df[municipality_column].dropna().astype(str).tolist()\n",
        "\n",
        "    return municipalities"
      ],
      "metadata": {
        "id": "308G5lKSa6WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# ロガーの設定 (force=True を使用)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler(out_file_txt, 'w', encoding='utf-8'),  # ファイルへのハンドラ (エンコーディングを指定)\n",
        "    ],\n",
        "    force=True  # 既存のハンドラーを強制的に削除\n",
        ")\n",
        "\n",
        "# ログ出力\n",
        "for doc in anonymized_docs:\n",
        "    logging.info(doc)\n",
        "# for doc1 in anonymized_docs1:\n",
        "#     logging.info(doc1)\n",
        "# for doc2 in anonymized_docs2:\n",
        "#     logging.info(doc2)"
      ],
      "metadata": {
        "id": "Qty4f-K40qZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec9db18-8855-49ef-bed6-8401ba836671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[7.88s] 病院の先生とかから何か診断された結果が、そこで刺されたのだろうという結果だということですかね。\n",
            "[20.82s] それで年末年始で病院も空いていないし、ホテルさんも病院も紹介してくれなかったので、すぐに迅速な対応もしていただけなかったんです。\n",
            "[38.22s] それから予約前から、お掃除だけは毎日お願いしますねと言ったら、滞りなくするので、安心してくださいというメールまでいただいているんですね、行く前に。\n",
            "[46.02s] それでもお掃除もできていないし、手術交換もなさっていないということもありまして。\n",
            "[50.52s] これ楽天さんのホテルの手配旅行とか募集型というお話されていたんですけど、\n",
            "[56.40s] もともとお送りいただいた内容は手配旅行と募集型とどちらだったんですかね、結果的に。\n",
            "[63.02s] それがどちらともわからなかったんですけど、向こうは手配旅行だと言い張るんですが、\n",
            "[77.24s] 予約した時にも楽天月末セールと書いてありますし、MbyqmVfeAOf9さんがお付けになったお値段かなと、お付けになって企画なさったのかなと、\n",
            "[80.58s] こちらも誤解するような。\n",
            "[92.58s] とも書いてありますし、予約確認、それから前日の確認、そこでも手配旅行という文字はどこにも見当たらなかったんですね。\n",
            "[109.14s] 普通、手配旅行が宿泊客側にデメリットが多いということがあるようなんですけど、そういうことも、そういう念押しもなく、\n",
            "[114.40s] 予約をさせるようなシステムだったので。\n",
            "[120.80s] 内容の方は大変な思いをされたということは、今お伺いされました。\n",
            "[131.72s] 今、消費者様はずっとこの継続されている中で、楽天さんの方に、もしくはホテル側さんの方にお話をどのようにされて、\n",
            "[136.10s] 何を今お話しとして、お年どころとしてお考えになられているということでございますか。\n",
            "[139.36s] そうですね、ホテルさんだったら、\n",
            "[140.80s] こちらは全然、\n",
            "[144.82s] 別に当たり前に、普通に綺麗に掃除がされていて、\n",
            "[148.82s] 普通にお出迎えしてもらって、\n",
            "[152.82s] 楽しかった、ありがとうって言って帰りたかったんですね。\n",
            "[153.82s] はい。\n",
            "[163.82s] でも、お掃除とか、スタッフの対応とか、親切な対応とかって、当たり前じゃないですか。\n",
            "[170.46s] だから、別に上のもっといい部屋に帰ってくれって言ったら、\n",
            "[170.78s] うん。\n",
            "[179.78s] 言ったりもしてませんし、理不尽なクレームも出していませんし、ちょっと、信号が汚れてたんですけど、とか、とか。\n",
            "[186.78s] 消費者様は、ホテル側さんと楽天さんと両方と話されていらっしゃるということですかね。\n",
            "[197.48s] 最初、楽天に問い合わせたんですけど、楽天は、うちは手配旅行だから、予約が完了したら、うち知りませんというような返事が返ってきて、\n",
            "[200.74s] ホテルに連絡をしたら、\n",
            "[202.78s] 1ヶ月以上経って・・・。\n",
            "[203.78s] はい。\n",
            "[205.78s] 調査にも1ヶ月以上経って・・・。\n",
            "[207.78s] はい。\n",
            "[210.78s] 1ヶ月以上経ってから・・・。\n",
            "[226.78s] 経ってるし、それに、滞在時にも、マシン、信号が、とか、加湿器とか、だから、滞在中からも、設備がちゃんと動かないとか、部屋、信号がちょっと汚れているとか。\n",
            "[227.78s] はい。\n",
            "[228.78s] はい。\n",
            "[229.78s] はい。\n",
            "[230.78s] はい。\n",
            "[238.42s] シーツが前のままだったとかそういう判定 家族を無心さされているとかそういうことを申し上げていたんですけど\n",
            "[241.82s] チェックアウトの時にも\n",
            "[247.76s] 精算もされないしで楽園トラベルさんで 最終\n",
            "[253.78s] あのあの時こう 飛行機の事故があったりして結構し\n",
            "[261.38s] 自信もあったりして新幹線も止まってて 帰るにも帰れなかったので\n",
            "[270.02s] 最終日はちょっと新幹線が取れたので だから1日分キャンセルしましたんですね\n",
            "[275.22s] ちょっとあのごめんなさい 話のポイント 要点に関しては こちらのご相談というのは\n",
            "[279.02s] 何を当協会の方にお望みでいらっしゃいますかね\n",
            "[283.62s] その対応が迅速ではなかったということと\n",
            "[283.76s] えぇ\n",
            "[290.44s] チェックアウト時に精算もしなかった それと楽天トラベルと\n",
            "[295.80s] カードの引き落としとの値段 金額も違う\n",
            "[298.52s] キャンセル えーと\n",
            "[304.46s] 4泊 1月29日から1月4日\n",
            "[311.62s] までの滞在の予定だったんですけど最終日を キャンセルしました ホテル側の落ち度だったんで\n",
            "[313.60s] キャンセル はい\n",
            "[316.20s] 小さくなってしまいました 聞こえなくて\n",
            "[318.24s] あのー えーと\n",
            "[322.40s] ですからホテルで連泊をしました\n",
            "[327.26s] 12月の29日から1月の4日の予定でしたが\n",
            "[331.94s] 冒頭でお伺いしたので連泊してるんですけども 当協会の方にご連絡いただいたのは\n",
            "[337.68s] ですから返金処理もしてくれないし wTnOB4mEr5Vbも逃げ回っているので\n",
            "[343.44s] ホテルもノラリプラリして対応をしてくれない ですから困ってるんです\n",
            "[348.60s] 返金はされない クレジットカードの明細と\n",
            "[352.40s] 0mUJrb1RneiVの履歴とも金額も合わないし\n",
            "[356.70s] 返金というのは最後の1泊の返金ということでございますか\n",
            "[359.04s] 1泊の返金と\n",
            "[362.40s] それからクレームの初動対応で\n",
            "[365.00s] 普通大きさ減少性格確認\n",
            "[367.80s] 具体的に確認する\n",
            "[370.80s] クレームに対して謝罪する\n",
            "[373.44s] 相手が望んでいる対応を確認するというのが\n",
            "[377.44s] クレーム初動対応らしいんですけど\n",
            "[383.44s] チェックアウトの時にもこれなかったんですよね\n",
            "[387.44s] ご返金の金額を改めてご確認ですけど\n",
            "[390.44s] 最後の1泊の分額はご返金がないという話ですか\n",
            "[394.44s] 最後の1泊もないし\n",
            "[396.44s] それから1ヶ月\n",
            "[402.44s] 最後の1泊はお取り消し料が発生してしまっているとかそういうことはないですか\n",
            "[408.44s] キャンセル料等の説明もありませんし\n",
            "[410.44s] 通常はあると思いますよ\n",
            "[413.44s] インターネットの方で楽天さんの方で予約された時に\n",
            "[415.44s] いついつから手数料がかかりますということが\n",
            "[420.44s] ウェブ上での案内はなかったんですか\n",
            "[423.44s] すぐ分かるところにはなかったですし\n",
            "[427.44s] ホテルの落ち度にもかかわらず\n",
            "[430.44s] それでもやはり同じように\n",
            "[433.44s] お話の内容を理解したんですけれども\n",
            "[435.44s] 今日の申し出の内容だと\n",
            "[438.44s] 当協会の方で何かを間に入って\n",
            "[443.44s] 紛争解決をさせていただけるという内容には至らない状況なんですね\n",
            "[444.44s] というのはですね\n",
            "[447.44s] 手配旅行ではだというふうにどこにも書いてなかったとか\n",
            "[450.44s] キャンセルチャージがどこにも見当たらなかったとか\n",
            "[452.44s] 消費者の方は結構その予約\n",
            "[455.44s] インターネットのサイトで申し込みされている方に結構そのお話されるんですけど\n",
            "[459.44s] 結果的には楽天さんも含めて各旅行会社さんの方で\n",
            "[461.44s] インターネットの申し込みに関してはですね\n",
            "[463.44s] 記載の方は\n",
            "[465.44s] 各社様でされていらっしゃるんですけど\n",
            "[467.44s] 結果的に我々は\n",
            "[470.44s] 消費者の方がどこにも書いてなかったと言われてしまうと\n",
            "[472.44s] それ以上の解決ができなくてですね\n",
            "[475.44s] 当然今いただいている消費者様以外に\n",
            "[478.44s] OTMzvCs2485Jさんのホームページを利用されて\n",
            "[479.44s] 予約をされている方っていうのも\n",
            "[481.44s] ものすごい数の方がいらっしゃる中でですね\n",
            "[484.44s] こういったトラブルが頻繁に起きているようであれば\n",
            "[487.44s] 当然何かの問題が生じていると思うんですけども\n",
            "[489.44s] そういったご意見をいただくケースって\n",
            "[492.44s] 非常に稀であるかないかぐらいなんですね\n",
            "[493.44s] そうですか\n",
            "[496.44s] UDYqy1qjDYceで検索するといっぱい出てくるんですけど\n",
            "[498.44s] それは口コミとかそういったことは\n",
            "[500.44s] 我々は全く検知しておりませんので\n",
            "[502.44s] それとこれとは話は別でございます\n",
            "[504.44s] 我々の協会の方にお問い合わせいただく際に\n",
            "[506.44s] ご相談する中では\n",
            "[509.44s] こういったトラブルのお話が非常に極めて少ないという話でございます\n",
            "[511.44s] 口コミで書かれているというのは\n",
            "[515.44s] それは我々はもう全く論外で感知しておりませんので\n",
            "[518.44s] それは書きたい方が書かれているだけのお話でございますので\n",
            "[519.44s] そこの部分に関しては\n",
            "[521.44s] 争点を当てないでいただきたいんですけども\n",
            "[523.44s] インターネットの方で\n",
            "[525.44s] お申し込みされるには\n",
            "[526.44s] ウェブの契約というのは\n",
            "[529.44s] お客様ご本人様がしっかりとその契約内容を\n",
            "[532.44s] 手配料かどうかも分からなかったとお話なんですけど\n",
            "[533.44s] それをしっかりご理解された上で\n",
            "[535.44s] お申し込みいただかなければいけないのが\n",
            "[537.44s] インターネットの契約でございますので\n",
            "[539.44s] 何も書いてなかった\n",
            "[540.44s] 言ってくれなかったというのは\n",
            "[542.44s] 大変申し上げにくいんですけども\n",
            "[545.44s] 我々の方がちょっと会議できる内容ではないということと\n",
            "[548.44s] あとホテル側さんのサービスに関しては\n",
            "[551.44s] これホテル側の方の問題でございますので\n",
            "[555.26s] WjrAWZ537yIPさんお客様の方の部分は\n",
            "[556.26s] ご期待に添えなかったというのが\n",
            "[559.26s] ホテルのサービスのレベルのお話でございますので\n",
            "[562.26s] ここは楽天とはまた別のお話になってくるんですね\n",
            "[564.26s] それをホテル側さんの方に\n",
            "[566.26s] 口を上げてらっしゃっている中で\n",
            "[569.26s] お話が進まないというお話なんですけども\n",
            "[572.26s] ホテル側さんの方で送ったサービスに関しての\n",
            "[574.26s] 屋上対応に関しては\n",
            "[576.26s] 我々の方も単に感知できなくてですね\n",
            "[578.26s] 我々が感知できるのは\n",
            "[580.26s] あくまでも旅行をされた中での\n",
            "[582.26s] 旅行業者さんとの間での\n",
            "[585.26s] 何かトラブルがあった場合の解決方法でございますので\n",
            "[587.26s] もし今できるとしたら\n",
            "[589.26s] 今いただいた内容を楽天さんの方に\n",
            "[591.26s] お伝えさせていただいた上で\n",
            "[593.26s] vnMFTzheRQU3さんの方でもう一度その記録の内容を\n",
            "[595.26s] 確認とっていただいた上で\n",
            "[597.26s] お取り消しをされた分に関しての\n",
            "[599.26s] お取り消し量がどうなっているのか\n",
            "[601.26s] ホテル側さんの方からの\n",
            "[603.26s] 謝罪がどうなっているかという\n",
            "[605.26s] そこに関してのご回答を\n",
            "[607.26s] 調査していただくことは可能ではあると思うんですが\n",
            "[610.26s] ただホテル側さんのサービスレベルの対応とかですね\n",
            "[612.26s] お客様の苦情というのは\n",
            "[614.26s] lhePtxgM9OpZさんの方でも\n",
            "[615.26s] ホテル側さんの方でも\n",
            "[617.26s] ホテル側さんの方に\n",
            "[619.26s] 今後気をつけてくださいというような\n",
            "[621.26s] お話はできると思うんですけど\n",
            "[623.26s] 指導とかですね\n",
            "[625.26s] そういった強制的なことを何かするということは\n",
            "[627.26s] できる立ち位置はなかなかございませんので\n",
            "[629.26s] 手配旅行を最後に\n",
            "[631.26s] それを先ほどおっしゃっていただいた\n",
            "[633.26s] 手配旅行も終えてしまっている後は\n",
            "[635.26s] 実際のところは契約内容は\n",
            "[637.26s] お客様とホテル側さんの方に\n",
            "[639.26s] のみに移行されているので\n",
            "[641.26s] 解決をするとしたら\n",
            "[643.26s] ホテル側さんとのお話になってくるんですけども\n",
            "[644.26s] ただホテルの対応というのも\n",
            "[646.26s] 対お客様に対して\n",
            "[648.26s] 回答が出されているという風にも\n",
            "[650.26s] なっているようであれば\n",
            "[652.26s] それ以上の回答は望めませんし\n",
            "[654.26s] 我々の方にもしご相談いただいて\n",
            "[656.26s] できるとしたら楽天さんの方に\n",
            "[658.26s] お客様の情報をお伝えして\n",
            "[660.26s] 一度内容を調べいただいて\n",
            "[662.26s] どういう今進捗状況なのか\n",
            "[664.26s] これ以上何かできるのか\n",
            "[666.26s] できないのか\n",
            "[668.26s] その点に関して確認を取ったものを\n",
            "[670.26s] ご回答させてあげるということまでは\n",
            "[672.26s] させていただくことは可能なんですけど\n",
            "[674.26s] それ以上のことはちょっとご対応に関しては\n",
            "[680.65s] 難しくなってまいりますよ\n",
            "[682.65s] そうですか\n",
            "[686.41s] お手伝いできるとしたら\n",
            "[688.41s] XsVNGu3zwTH1さんの方にご連絡を\n",
            "[690.41s] お客様の名前等をお伺いして\n",
            "[692.41s] 予約番号を取っていただいて\n",
            "[694.41s] それを楽天さんの方にお伝えして\n",
            "[696.41s] もう一度調査していただいて\n",
            "[698.41s] お客様の方に楽天さんもしくは\n",
            "[700.41s] ホテル側さんの方から\n",
            "[702.41s] JACTAにもご連絡いただいたものも含めて\n",
            "[704.41s] 最終的にどういう見解なのか\n",
            "[706.41s] どういう状況なのかということを\n",
            "[708.41s] ご連絡していただくということぐらいは\n",
            "[710.41s] できるんですけども\n",
            "[712.41s] それ以上のことはちょっと難しいという\n",
            "[716.81s] お話でございますね\n",
            "[718.81s] オンライン旅行取引であるのは\n",
            "[720.81s] わかっておりますけれども\n",
            "[722.81s] 問い合わせ先に関する事項\n",
            "[724.81s] それから\n",
            "[726.81s] そういう不利益な事項というのは\n",
            "[728.81s] もっとわかりやすい場所にあるべきであって\n",
            "[730.81s] それが\n",
            "[732.81s] 予約完了の時にも\n",
            "[734.81s] 予約の最中にも\n",
            "[736.81s] わかりやすいところに\n",
            "[738.81s] 書かれていなかったというのは\n",
            "[740.81s] 問題じゃないんですか\n",
            "[742.81s] はい\n",
            "[744.81s] それは\n",
            "[746.81s] 各社様のやり方でございますよ\n",
            "[748.81s] 各社のやり方とは言っても\n",
            "[750.81s] インバウンドで\n",
            "[752.81s] コロナで大変だった時期ですよね\n",
            "[754.81s] そうやって\n",
            "[756.81s] 客を誤解させるような\n",
            "[758.81s] 記述をするというのは\n",
            "[760.81s] ちょっとおかしいんじゃないですか\n",
            "[762.81s] たくさん申し込みされるときに\n",
            "[764.81s] 同じようなトラブルが起きていると思うんですけど\n",
            "[766.81s] そういったようなお話は\n",
            "[768.81s] いただいていないんですけど\n",
            "[770.81s] xxFxuoYfTnzrの方では\n",
            "[772.81s] ものすごい多数の方が予約されていらっしゃるんですけど\n",
            "[774.81s] 同じようなトラブルが起きているのであれば\n",
            "[776.81s] 社会問題になるくらいのことが起きていると\n",
            "[778.81s] おかしくないと思うんですが\n",
            "[780.81s] そういったことが起きていないんですね\n",
            "[782.81s] 実際のところ\n",
            "[784.81s] トコジラミ流行ってますよね\n",
            "[786.81s] トコジラミは\n",
            "[788.81s] WZfVfeekzhNXだけでなく\n",
            "[790.81s] 各国で流行っていらっしゃるんですね\n",
            "[792.81s] そうですね\n",
            "[794.81s] 虫の被害\n",
            "[796.81s] も\n",
            "[798.81s] 何もされていないんだ\n",
            "[800.81s] というのと\n",
            "[802.81s] こちらが持ち込んだんじゃないか\n",
            "[804.81s] というようなことまで\n",
            "[806.81s] 言われて\n",
            "[808.81s] こちらも\n",
            "[810.81s] 年末年始の\n",
            "[812.81s] 息抜きの家族サービスで\n",
            "[814.81s] 泊まっていて\n",
            "[816.81s] それで\n",
            "[818.81s] 変なものを持ち込んだような\n",
            "[820.81s] 扱いまでされて\n",
            "[822.81s] それは\n",
            "[824.81s] 業者さんの方にお話するべきだと思います\n",
            "[826.81s] それも言いましたけれども\n",
            "[828.81s] 何とも思っていらっしゃらないようだったので\n",
            "[830.81s] 相談する場所がなく\n",
            "[832.81s] こちらに\n",
            "[834.81s] お電話させていただいているんですね\n",
            "[836.81s] サービス提供したのではないため\n",
            "[838.81s] いただいた内容に関して\n",
            "[840.81s] 客観的な状況の中で\n",
            "[842.81s] 手配どころということでございますので\n",
            "[844.81s] それ以上の\n",
            "[846.81s] やっかんに照らし合わせた内容に関しての\n",
            "[848.81s] 処理の方法はご案内できるんですけど\n",
            "[850.81s] 感情的なお話とか\n",
            "[852.81s] サービスの中身の内容というのは\n",
            "[854.81s] 各社様の方のお話になってくるので\n",
            "[856.81s] お話いただくとしたら\n",
            "[858.81s] dPk66D08e7NUさんの方につけていただくしかないと思いますので\n",
            "[860.81s] それで契約が\n",
            "[862.81s] 終わったからうち知りませんって\n",
            "[864.81s] 言い続けているんですけど\n",
            "[866.81s] だからこういう不親切な\n",
            "[868.81s] 記載もありますし\n",
            "[870.81s] それであたかも楽天さん\n",
            "[872.81s] 専用のツアーであるように\n",
            "[874.81s] 書かれているので\n",
            "[876.81s] こちらも誤解するので\n",
            "[878.81s] 何とも申し上げることはできませんけど\n",
            "[880.81s] kW6IqKX1Anuqさんの方が手配旅行の手配サメが\n",
            "[882.81s] 終わっていらっしゃるという会社の見解が\n",
            "[884.81s] 出されているのであれば\n",
            "[886.81s] それ以上我々の方がそこに介入することはできないと思います\n",
            "[888.81s] 手配旅行というのは\n",
            "[890.81s] 手配旅行がホテルの予約をしたという時点で\n",
            "[892.81s] ホテルの手配サメが\n",
            "[894.81s] 完了しているものでございますので\n",
            "[896.81s] それならば\n",
            "[898.81s] 注文完了\n",
            "[900.81s] するまでに\n",
            "[902.81s] 何らかの説明をするべきですし\n",
            "[904.81s] 役場の我々におっしゃっていただいた\n",
            "[906.81s] どう変えることもできないので\n",
            "[908.81s] USmwlK7TOB6qさんの方に\n",
            "[910.81s] ですから向こうに行っても\n",
            "[912.81s] 契約が終了しているからの一点張りで\n",
            "[914.81s] 話を聞いてくれないので\n",
            "[916.81s] こうやってご相談させていただいているんです\n",
            "[918.81s] 先ほどお話した通り\n",
            "[920.81s] こちらでも解決できない問題もございますので\n",
            "[922.81s] インターネットのウェブサイトの見方を\n",
            "[924.81s] 変えてくださいとか\n",
            "[926.81s] こうしてくださいああしてくださいということを\n",
            "[928.81s] 我々は言える立場にはございませんので\n",
            "[930.81s] ではどこにご相談したらよろしいですか\n",
            "[932.81s] 教えていただけませんか\n",
            "[934.81s] それは存じていないです\n",
            "[936.81s] 消費者センターさんとかそういったところになってくると思います\n",
            "[938.81s] ではなんでこんなに手配旅行だけを超えて\n",
            "[940.81s] 客が割り送るんですか\n",
            "[942.81s] 募集型企画旅行の場合でも同じでございますよ\n",
            "[944.81s] 手配旅行もそうですけど\n",
            "[946.81s] 募集型企画旅行の場合でも\n",
            "[948.81s] ホテル側さんの方のサービスの内容に関しては\n",
            "[950.81s] もちろんホテル側さんの\n",
            "[952.81s] 旅行会社さんの方も\n",
            "[954.81s] 間に入られることは入りますけれども\n",
            "[956.81s] ホテルで起こった問題に関しては\n",
            "[958.81s] ホテル側さんの方のお答えになってもらいますよ\n",
            "[960.81s] そうですね\n",
            "[962.81s] ホテルの一方的な過失であって\n",
            "[964.81s] こちらに過失はないんですけれども\n",
            "[966.81s] 今までこうやってのらりこらりと\n",
            "[968.81s] 対応してもらえない\n",
            "[970.81s] ホテル側さんもそういう誤解がされているのであれば\n",
            "[972.81s] 一ホテル側さんの方に\n",
            "[974.81s] 我々の協会から何か申し上げることはできないので\n",
            "[976.81s] これ以上のお答えは難しいという話になってもらいます\n",
            "[978.81s] 我々はほぼ限界です\n",
            "[980.81s] そうなってくると\n",
            "[982.81s] 消費者としては\n",
            "[984.81s] 相談するところがどこにもないから\n",
            "[986.81s] そうやって相談がないんじゃないですか\n",
            "[988.81s] ちょっと申し上げにくいんですが\n",
            "[990.81s] これ以上のお答えはできない\n",
            "[992.81s] もしされるのであれば\n",
            "[994.81s] あと消費生活センターさんとか\n",
            "[996.81s] そういったところにご相談いただくのは\n",
            "[998.81s] よろしいかと思いますが\n",
            "[1000.81s] 今の我々のycvHHjC7T2su旅行業協会という立ち位置の中で\n",
            "[1002.81s] 気配旅行債務とか\n",
            "[1004.81s] 募集型極力債務の役割の中での\n",
            "[1006.81s] お話をお伺いすると\n",
            "[1008.81s] お客様のお話を整理させていただくと\n",
            "[1010.81s] ちょっと困難だと思われますが\n",
            "[1012.81s] 先ほどお話したとおり\n",
            "[1014.81s] できるとしたら\n",
            "[1016.81s] RWCIkZJ6AxyGさんの方にご連絡をさせてあげて\n",
            "[1018.81s] もう一度お客様の方にご連絡してください\n",
            "[1020.81s] ということぐらいはできるんですが\n",
            "[1022.81s] それ以上のことは大変申し訳ないと\n",
            "[1024.81s] できないという状況でございます\n",
            "[1026.81s] それでは恐れ入りますが\n",
            "[1028.81s] 調査をお願いすることを\n",
            "[1030.81s] ご連絡していただけませんでしょうか\n",
            "[1032.81s] もちろんです\n",
            "[1034.81s] お客様はまずお名前をフルネームで\n",
            "[1036.81s] いただいてもよろしいですか\n",
            "[1038.81s] ElOlEzuT5tA6X4s5YaFXI0Mh\n",
            "[1040.81s] はい\n",
            "[1042.81s] 予約番号とか\n",
            "[1044.81s] PtSa4rZTZ1pmさんの時で予約していた予約番号\n",
            "[1050.30s] ございますか\n",
            "[1052.30s] はいございます\n",
            "[1057.59s] 予約番号は\n",
            "[1059.59s] えーっと\n",
            "[1061.59s] アルファベット\n",
            "[1063.59s] 大文字のR\n",
            "[1065.59s] R\n",
            "[1067.59s] 大文字のY\n",
            "[1069.59s] Y\n",
            "[1071.59s] 小文字のA\n",
            "[1073.59s] A\n",
            "[1075.59s] 0\n",
            "[1077.59s] 0\n",
            "[1079.59s] 小文字のHが2つ\n",
            "[1081.59s] HH\n",
            "[1083.59s] 小文字のJ\n",
            "[1085.59s] J\n",
            "[1087.59s] arDYyzPTBDdjのJでよろしいですか\n",
            "[1089.59s] GKIZzjVzBfbMのJです\n",
            "[1091.59s] 数字で46\n",
            "[1093.59s] 46\n",
            "[1095.59s] 最後がHです\n",
            "[1097.59s] 最後の文字どれくらいですか\n",
            "[1099.59s] 小文字でHです\n",
            "[1101.59s] Hですね\n",
            "[1105.59s] 小文字のRYから始まりまして\n",
            "[1107.59s] 小文字のA\n",
            "[1109.59s] 数字の0\n",
            "[1111.59s] 小文字のHH\n",
            "[1113.59s] 小文字のJのJ\n",
            "[1115.59s] 数字の46\n",
            "[1117.59s] 小文字のH\n",
            "[1119.59s] では間違いないですがね\n",
            "[1121.59s] u1USfhujbXuY61Kh4nfIrxAK様で12月29日から\n",
            "[1123.59s] ご出発されたホテルです\n",
            "[1125.59s] ホテルのお名前はどちらになりますか\n",
            "[1127.59s] ホテル\n",
            "[1129.59s] モントレ\n",
            "[1131.59s] xxW9Ze8474DgRJ8yTEO0cYdG\n",
            "[1133.59s] ホテルモントレE\n",
            "[1135.59s] ホテルモントレ\n",
            "[1137.59s] ホテルで\n",
            "[1139.59s] マミムメモノモ\n",
            "[1141.59s] 最後のN\n",
            "[1143.59s] モントレ\n",
            "[1145.59s] モントレ\n",
            "[1147.59s] モントレ\n",
            "[1149.59s] oKhez7XdqEUxiIyAGekGX8x7\n",
            "[1151.59s] Po6UVP1xodJQBCWDggdPfGhY\n",
            "[1153.59s] DEH6qPDrzSZZ1Jd5ZVEKBgWL\n",
            "[1155.59s] skcUgBMAnPhopFBuTpA8CFnl\n",
            "[1157.59s] U4oA8fYaFVyNOeYEgAOoR0uM\n",
            "[1159.59s] ちょっと調べればわかります\n",
            "[1161.59s] はい\n",
            "[1167.59s] fnUm0dwyC2iMさんのやりとりされる方のお名前とかお分かりになりますか\n",
            "[1180.90s] p0jqpWScR7Gu様\n",
            "[1187.74s] ホテルは\n",
            "[1193.74s] 8bwTJrhFsMvEdMhWSqbpPCMD\n",
            "[1197.62s] 8lGOgIRyyYoTDziCJ0snwMx0様\n",
            "[1198.66s] ef548s1gwUNXpB2nWCihVo6i様ですね\n",
            "[1199.54s] あとT9eQT5UOG9gb\n",
            "[1201.28s] QdZRsnXfyE8NJm9oDhDkihAA\n",
            "[1203.50s] ZClO1pEl2PwbGWTEAiBBfZNO様\n",
            "[1204.62s] uZSUGTATGylZ\n",
            "[1208.42s] kZi35XONpF5c様でよろしいですかね\n",
            "[1210.06s] はい\n",
            "[1213.38s] まずお客様ですね申し訳ないのに先ほど冒頭でお話した通り\n",
            "[1218.60s] 当業界はですね会員であるのはあくまでも会員の一つの一社様でありましてですね\n",
            "[1221.24s] あのホームページの方にも記載があるんですけど\n",
            "[1223.18s] 公平公正の立場で\n",
            "[1229.34s] あの旅行業会社さんとの間に紛争が起きた場合での解決をさせていただく相談窓口でございまして\n",
            "[1233.90s] その先にあるサービス提供機関のホテル側の方で起こっているサービスの問題は\n",
            "[1235.74s] 介入できないものでございます\n",
            "[1240.76s] ただ旅行会社さんを通して予約されているということで楽天さんを一つ間に入ってますので\n",
            "[1244.60s] 今いただいた内容を楽天さんの方にお伝えした上で\n",
            "[1246.06s] 進捗状況も含めて\n",
            "[1249.74s] さらにQ296ufG2G7re様よりお電話をいただいた\n",
            "[1253.54s] 当業界にお電話をいただいたので改めて内容の方を確認とっていただいて\n",
            "[1258.44s] 何かすることができないのかなどを含めて確認を取ることが可能でございます\n",
            "[1259.32s] はいぜひ\n",
            "[1261.68s] ぜひ対応お願いいたします\n",
            "[1267.50s] これに関して指導とかそういった強制権というのは申し上げておりませんので\n",
            "[1270.06s] このことをご理解いただいた上でですね\n",
            "[1272.74s] こちらのご対応と返させていただきます\n",
            "[1274.72s] ご対応でございますか\n",
            "[1278.52s] PmpZOG0K8j9Aと申しますので内容の方をお伺いさせていただきましたので\n",
            "[1280.86s] はいsndFirSNHwBw様ありがとうございます\n",
            "[1285.54s] ではこういう時にはどうやって対策をすればよろしいですか\n",
            "[1288.60s] 対策と申しますと\n",
            "[1289.98s] そうですね\n",
            "[1293.72s] 見てない方が悪いって言われましても\n",
            "[1297.62s] こうやって分かりやすい形状をしていない方が悪いですし\n",
            "[1298.64s] 分かりやすい表示かどうかというのは\n",
            "[1300.88s] 消費者様の見方によって様々な\n",
            "[1305.16s] 見方によって様々になるような表示をするなということなんですよ\n",
            "[1309.24s] XQe5wMHPSNjH様我々はそれに関してどうすればいいということを申し上げることはできないです\n",
            "[1314.18s] 各旅行会社の方のやり方の方法に関しましては言及はできないので\n",
            "[1316.08s] 失礼いたしました分かりました\n",
            "[1318.02s] ではQhFhRn9YJkeTの方に\n",
            "[1320.48s] 連絡だけお願いできますか\n",
            "[1322.98s] ちょっとお話聞いていただいてもいいですか\n",
            "[1328.10s] お電話番号は080-6220-9818でよろしいですかね\n",
            "[1329.36s] はいさようでございます\n",
            "[1333.00s] あとですねお時間を確認取るのに調査するお時間も\n",
            "[1334.54s] 多分各社さんもあると思いますので\n",
            "[1338.04s] このお電話の直後にお電話するできるというお約束はできません\n",
            "[1340.16s] 場合によっては数日\n",
            "[1341.66s] 今日月曜日でございますが\n",
            "[1344.12s] 数日2,3日かかる可能性もございますので\n",
            "[1345.82s] その点はご理解いただければと思います\n",
            "[1348.00s] この後私の方から楽天さんにお話を聞いていただきたいと思います\n",
            "[1351.08s] 楽天の方も担当の方に内容をお伝えさせていただいて\n",
            "[1355.92s] jZTExXWu2lZ8様の方に近日中にご連絡するように申し伝えたいと思いますので\n",
            "[1359.28s] 恐れ入りますがよろしくお願い申し上げます\n",
            "[1360.60s] よろしくお願いいたします\n",
            "[1374.52s] よろしくお願いします\n",
            "[1374.78s] はい\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Patterns for sensitive information\n",
        "patterns_ascii = [\n",
        "  r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",  # Matches email addresses\n",
        "  r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",  # Matches IP addresses\n",
        "  r\"\\d{3}-?\\d{4}\",  # Matches Japanese postcodes\n",
        "  r\"https?://[^\\s]+\" # Matches URLs\n",
        "]\n",
        "pattern_alphabet_name = [r\"[a-zA-Z]{2,}\" ]# Matches names (at least two words)"
      ],
      "metadata": {
        "id": "st_o-e53jQrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "より安全でスマートな方法として、secrets モジュールを使用することをお勧めします。\n",
        "\n",
        "理由：\n",
        "\n",
        "secrets モジュールは、暗号学的に安全な乱数を生成するために設計されています。\n",
        "random モジュールよりも予測不可能な乱数を生成するため、セキュリティの観点からより安全です。\n",
        "secrets.choice() 関数を使用することで、指定された文字セットからランダムな文字を選択できます。"
      ],
      "metadata": {
        "id": "rQtOktUEw8I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import secrets\n",
        "import string\n",
        "\n",
        "def generate_secure_random_string(length=12):\n",
        "    \"\"\"Generate a secure random string of specified length.\"\"\"\n",
        "    alphabet = string.ascii_letters + string.digits\n",
        "    return ''.join(secrets.choice(alphabet) for i in range(length))\n",
        "\n",
        "def anonymize_with_pattern(patterns, text):\n",
        "    \"\"\"\n",
        "    Replaces matches of patterns with random strings,\n",
        "    excluding those in brackets with specific time formats.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Protect numbers inside brackets with a temporary placeholder\n",
        "    # (Before replacing brackets with special symbols)\n",
        "    bracketed_numbers = re.findall(r\"(?:\\d+[:.]\\d+|\\d+[hms])\", text) # Changed pattern to match original brackets\n",
        "    # print(\"aaa\", bracketed_numbers)\n",
        "    for i, num in enumerate(bracketed_numbers):\n",
        "        text = text.replace(num, f\"[_PROTECTED_NUMBER_{i}]\", 1)  # Changed placeholder format\n",
        "\n",
        "    # 2. Replace brackets with special symbols to avoid regex conflicts\n",
        "    text = text.replace(\"[\", \"｟\")\n",
        "    text = text.replace(\"]\", \"｠\")\n",
        "    # print(\"sss\" , text)\n",
        "\n",
        "    # 3. Anonymize the text, excluding protected numbers\n",
        "    for pattern in patterns:\n",
        "        # Skip the exclusion pattern (if any)\n",
        "        if pattern.startswith(\"!\"):\n",
        "            continue\n",
        "\n",
        "        # Find all matches of the current pattern\n",
        "        matches = list(re.finditer(pattern, text))\n",
        "\n",
        "        # Iterate through matches in reverse order to avoid index issues\n",
        "        for match in reversed(matches):\n",
        "            # Replace the match with a random string\n",
        "            text = text[:match.start()] + generate_secure_random_string() + text[match.end():]\n",
        "\n",
        "    # 4. Restore the protected numbers\n",
        "    for i, num in enumerate(bracketed_numbers):\n",
        "        text = text.replace(f\"｟_PROTECTED_NUMBER_{i}｠\", num, 1)\n",
        "\n",
        "    # 5. Revert special symbols back to brackets\n",
        "    text = text.replace(\"｟\", \"[\")\n",
        "    text = text.replace(\"｠\", \"]\")\n",
        "    return text\n",
        "\n",
        "# Patterns for sensitive information\n",
        "patterns_phone = [\n",
        "    r\"\\b0\\d{2,3}(-\\d{1,4}-\\d{4}|\\d{8})\\b\",  # Full phone numbers with or without hyphens\n",
        "    r\"\\b0\\d{2,3}-\\d{1,4}\\b\",  # Partial phone numbers (e.g., 080-1234)\n",
        "    r\"\\b0\\d{2,3}\\b\",  # Partial phone numbers (e.g., 080)\n",
        "    r\"(?<![\\d｟])\\d{4}(?![\\d｠])\",  # Matches 4-digit numbers not preceded or followed by other digits or brackets\n",
        "    r\"(?<![\\d｟])\\d{2}(?![\\d｠])\"   # Matches 2-digit numbers not preceded or followed by other digits or brackets\n",
        "]"
      ],
      "metadata": {
        "id": "QaQBZhqRo7yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"[1:23:45] お電話番号をどうぞ。[100.4s]\\n070\\nはい\\n1234\\nはい\\n5678\\nありがとうございます。070-3456-7890ですね\\nちがいます。070-1234-5678です。はい34と 56ですね\"\n",
        "anonymized_text = anonymize_with_pattern(patterns_phone, text)\n",
        "print(anonymized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_mpXexg0cx0",
        "outputId": "310d3f72-5195-494a-ac9c-4298562465c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1:23:45] お電話番号をどうぞ。[100.4s]\n",
            "KaJGsB5dTqY3\n",
            "はい\n",
            "BD5dyFSFrx7k9zyjttJfl9\n",
            "はい\n",
            "a5ety5LQ9hcm\n",
            "ありがとうございます。FElIcbO4entc2bCHFu0GSx-ehkeyq3XrHIlですね\n",
            "ちがいます。md4W5MyCkRlQK6g7MjqNp4-o5Pv1sDl1QT8です。はい4lZGsS4QEeA5と KvKHAMKmyLjCですね\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "テスト用"
      ],
      "metadata": {
        "id": "4iqgTyrjypYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate_random_stringは、最初の関数がおすすめです。\n",
        "\n",
        "理由：\n",
        "\n",
        "よりランダム性が高い: 最初の関数は、大文字と数字の両方を使用するため、生成される文字列のバリエーションが豊富になり、よりランダム性が高まります。2番目の関数は大文字のみを使用するため、ランダム性が低くなります。\n",
        "より安全: 最初の関数は、より多くの文字を使用するため、生成される文字列が推測されにくくなり、より安全です。\n",
        "結論：\n",
        "\n",
        "よりランダム性と安全性を考慮すると、最初の関数の使用が推奨されます。"
      ],
      "metadata": {
        "id": "WUP4ICUfwjrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def anonymize_doc(doc):\n",
        "#   \"\"\"Anonymizes a spaCy Doc object by replacing sensitive tokens with anonymized versions.\"\"\"\n",
        "#   new_tokens = []\n",
        "#   print(doc)\n",
        "#   # Store the whitespaces of original tokens\n",
        "#   whitespaces = [token.whitespace_ for token in doc]\n",
        "#   for token in doc:\n",
        "#     if 'PRO' in token.pos_:  # Check if the token is a pronoun\n",
        "#       print(token.text,token.pos_)\n",
        "#       # new_tokens.append(generate_secure_random_string())\n",
        "#     # elif 'NUM' in token.pos_:  # Check if the token is a number\n",
        "#       pattern_num = r\"(?:\\d+[:.]\\d+|\\d+[hms])\"\n",
        "#       compiled_pattern = re.compile(pattern_num, re.UNICODE)\n",
        "#       print(compiled_pattern)\n",
        "#       if compiled_pattern:\n",
        "#         print(token.text,token.pos_)\n",
        "#       pattern = r\"^[^ -~｡-ﾟ]+$\"\n",
        "#       compiled_pattern2 = re.compile(pattern_num, re.UNICODE)\n",
        "#       if compiled_pattern2:\n",
        "#         print(token.text,token.pos_)\n",
        "#         # new_tokens.append(generate_secure_random_string())\n",
        "#     else:\n",
        "#       new_tokens.append(token.text)  # Keep original token\n",
        "\n",
        "#   # Create a new Doc with anonymized tokens\n",
        "#   new_doc = spacy.tokens.Doc(doc.vocab, words=new_tokens, spaces=whitespaces) # using spaces parameter\n",
        "#   return new_doc"
      ],
      "metadata": {
        "id": "aL4rwxBG8rNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Convert the iterable to a list\n",
        "# anonymized_docs = [anonymize_with_pattern(patterns_phone, doc.text) for doc in list_of_docs]  # Convert the iterable to a list\n",
        "\n",
        "# # Change here: Access the .text attribute of the Doc object before passing it to the function\n",
        "#The original problem is that you're applying the anonymization twice, first in the creation of\n",
        "#anonymized_docs and then again in anonymized_docs1. Since anonymized_docs is already a list of strings,\n",
        "#you should apply anonymize_interview_data_ascii directly to those strings.\n",
        "\n",
        "patterns_ascii = [\n",
        "    r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",  # Email addresses\n",
        "    r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\",  # IP addresses\n",
        "    # Add other patterns as needed\n",
        "]\n",
        "\n",
        "anonymized_docs2 = [anonymize_interview_data_ascii(doc, patterns_ascii) for doc in anonymized_docs1]\n"
      ],
      "metadata": {
        "id": "RLxisC3sxjZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Anonymizes Japanese names\n",
        "def anonymize_interview_data(text):\n",
        "    \"\"\"\n",
        "    Anonymizes interview data by replacing sensitive information with random strings.\n",
        "    Args:\n",
        "        text (str): The interview text to anonymize.\n",
        "    Returns:\n",
        "        str: The anonymized interview text.\n",
        "    \"\"\"\n",
        "\n",
        "    # # Patterns for sensitive information (add or modify as needed)\n",
        "    # patterns = {\n",
        "    #     \"address\": r\"[\\w\\s,-]+\",  # Matches general address patterns\n",
        "    #     \"institution\": r\"[^\\s]+(?:\\s[^\\s]+)+\",  # Matches institution names (at least two words)\n",
        "    #\n",
        "\n",
        "    # Replace municipality names using the create_list_municipalities function\n",
        "    municipalities = create_list_municipalities('./20240101現在の団体.csv')\n",
        "    municipalities.sort(key=len, reverse=True)\n",
        "    for municipality in municipalities:\n",
        "        text = re.sub(re.escape(municipality), generate_secure_random_string(), text)\n",
        "\n",
        "    # # Anonymize other sensitive information using the defined patterns\n",
        "    # for pattern_type, pattern in patterns.items():\n",
        "    #     compiled_pattern = re.compile(pattern)  # Compile pattern for efficiency\n",
        "    #     # text = anonymize_pattern(compiled_pattern, text)\n",
        "    #     text = anonymize_with_pattern(compiled_pattern, text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "pwLzvPGxgxDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anonymize_doc(doc):\n",
        "  \"\"\"Anonymizes a spaCy Doc object by replacing sensitive tokens with anonymized versions.\"\"\"\n",
        "  new_tokens = []\n",
        "  # Store the whitespaces of original tokens\n",
        "  whitespaces = [token.whitespace_ for token in doc]\n",
        "  # Anonymize tokens based on their POS tags\n",
        "  for token in doc:\n",
        "    if token.pos_ == 'PRON': # Replace 'PRON' with the actual POS tag for pronouns in your data\n",
        "      print(token.pos_, token.text)\n",
        "      # Anonymize pronouns, ensuring a non-empty string is returned\n",
        "      anonymized_token = generate_secure_random_string()\n",
        "      # If anonymized_token is empty, keep the original token\n",
        "      new_tokens.append(anonymized_token if anonymized_token else token.text)\n",
        "    else:\n",
        "      new_tokens.append(token.text)  # Keep original token\n",
        "\n",
        "  # Create a new Doc with anonymized tokens\n",
        "  # Handle case where new_tokens is empty\n",
        "  if new_tokens:\n",
        "    new_doc = spacy.tokens.Doc(doc.vocab, words=new_tokens, spaces=whitespaces)\n",
        "  else:\n",
        "    # Return original doc or handle appropriately\n",
        "    new_doc = doc  # Or raise an exception, log a message, etc.\n",
        "\n",
        "  return new_doc"
      ],
      "metadata": {
        "id": "xSa41_4PLLwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def anonymize_interview_data_ascii(text, patterns):\n",
        "    \"\"\"\n",
        "    Anonymizes interview data by replacing sensitive information with random strings.\n",
        "    Focuses on numeric, alphabets, and patterns like email addresses and URLs.\n",
        "    Args:\n",
        "        text (str): The interview text to anonymize.\n",
        "    Returns:\n",
        "        str: The anonymized interview text.\n",
        "    \"\"\"\n",
        "    # Anonymize using the defined patterns\n",
        "    for pattern in patterns:\n",
        "        # This ensures the pattern treats the string as unicode\n",
        "        # This may help in correctly interpreting double-byte characters.\n",
        "        # Compile the pattern with the re.UNICODE flag\n",
        "        compiled_pattern = re.compile(pattern, re.UNICODE)\n",
        "        # print(compiled_pattern)\n",
        "\n",
        "        # Pass the original pattern and the compiled pattern to anonymize_with_pattern\n",
        "        text = anonymize_with_pattern_ascii(pattern, compiled_pattern, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def anonymize_with_pattern_ascii(pattern, compiled_pattern, text):\n",
        "    \"\"\"\n",
        "    Anonymizes text using the provided patterns.\n",
        "\n",
        "    Args:\n",
        "        pattern (str): The original pattern string.\n",
        "        compiled_pattern (_sre.SRE_Pattern): The compiled regex pattern.\n",
        "        text (str): The text to anonymize.\n",
        "\n",
        "    Returns:\n",
        "        str: The anonymized text.\n",
        "    \"\"\"\n",
        "    # Skip the exclusion pattern (if any)\n",
        "    if pattern.startswith(\"!\"):\n",
        "        return text  # Return the original text if it's an exclusion pattern\n",
        "\n",
        "    # Perform anonymization using the compiled pattern\n",
        "    # ... (your existing logic to anonymize text using compiled_pattern) ...\n",
        "\n",
        "    return text  # Return the anonymized text"
      ],
      "metadata": {
        "id": "WSJL4rjv1jhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the breakdown of the issue:\n",
        "\n",
        "anonymized_docs is a list of strings, as revealed by the global variables you provided: anonymized_docs value: 4 items ['[435.44s] 今日の申し出の内容だ…', ... ,type: list.\n",
        "The list comprehension iterates through each item (doc) in anonymized_docs.\n",
        "Inside the comprehension, you attempt to access doc.text, but doc is a string, not a spaCy Doc object (which would have a text attribute). This is why the AttributeError is raised.\n",
        "Essentially, you are expecting anonymized_docs to be a list of spaCy Doc objects, but it's actually a list of strings, likely already processed text.\n",
        "\n"
      ],
      "metadata": {
        "id": "GkWnvlUVw9KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最終的な推奨コード"
      ],
      "metadata": {
        "id": "kXeuEh8B0vWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "「解決策1：force=True を使用する」方がおすすめです。\n",
        "\n",
        "理由\n",
        "\n",
        "force=True を使用することで、既存のハンドラーを強制的に削除し、新しいハンドラーを設定することができます。これにより、ハンドラーの競合を防ぎ、ログが正しくファイルに書き込まれることを保証できます。\n",
        "logging.shutdown() を使用してバッファをフラッシュする方法は、ログ出力の最後にのみ実行する必要があります。force=True を使用すると、この手順を省略することができます。\n",
        "force=True は Python 3.8 以降で利用可能であり、より新しいバージョンで推奨される方法です。"
      ],
      "metadata": {
        "id": "ga1TK9x60iid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format='%(message)s',\n",
        "#     handlers=[\n",
        "#         logging.StreamHandler(),\n",
        "#         logging.FileHandler('output.txt', 'w', encoding='utf-8'),  # ファイルへのハンドラ (エンコーディングを指定)\n",
        "#     ],\n",
        "#     force=True  # 既存のハンドラーを強制的に削除\n",
        "# )\n",
        "\n",
        "# # ログ出力\n",
        "# for doc in anonymized_docs2:\n",
        "#     logging.info(doc)"
      ],
      "metadata": {
        "id": "TZ7PofsGz81E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "解決策2：ロガーを明示的に取得する で同じ行が二度出力されたとのこと、申し訳ございません。これは、ルートロガーと明示的に取得したロガーの両方にハンドラーが設定されているため、ログが重複して出力されたことが原因と考えられます。"
      ],
      "metadata": {
        "id": "oOZFwH3e0m06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "\n",
        "# # ロガーの取得\n",
        "# logger = logging.getLogger(__name__)  # 現在のモジュール名を使用\n",
        "# logger.setLevel(logging.INFO)\n",
        "\n",
        "# # ハンドラーの設定\n",
        "# file_handler = logging.FileHandler('output.txt', 'w', encoding='utf-8')\n",
        "# file_handler.setLevel(logging.INFO)\n",
        "# file_handler.setFormatter(logging.Formatter('%(message)s'))\n",
        "\n",
        "# stream_handler = logging.StreamHandler()\n",
        "# stream_handler.setLevel(logging.INFO)\n",
        "# stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
        "\n",
        "# # ロガーにハンドラーを追加\n",
        "# logger.addHandler(file_handler)\n",
        "# logger.addHandler(stream_handler)\n",
        "\n",
        "# # ログ出力\n",
        "# for doc in anonymized_docs2:\n",
        "#     logger.info(doc)"
      ],
      "metadata": {
        "id": "Z4mUu38Qzq1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "\n",
        "# # ロガーの設定 (force=True を使用)\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format='%(message)s',\n",
        "#     handlers=[\n",
        "#         logging.StreamHandler(),\n",
        "#         logging.FileHandler('output.txt', 'w', encoding='utf-8'),  # ファイルへのハンドラ (エンコーディングを指定)\n",
        "#     ],\n",
        "#     force=True\n",
        "# )\n",
        "\n",
        "# # ログ出力\n",
        "# for doc in anonymized_docs2:\n",
        "#     logging.info(doc)\n",
        "\n",
        "# # バッファをフラッシュ\n",
        "# logging.shutdown()  # すべてのハンドラーをクローズし、バッファをフラッシュ"
      ],
      "metadata": {
        "id": "-KwCo_R-v0l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "\n",
        "# # print(spacy.prefer_gpu()) # Trueと表示されれば, GPUが使用されます．\n",
        "# if(spacy.prefer_gpu()):\n",
        "#   nlp = spacy.load('ja_ginza_electra')\n",
        "# else:\n",
        "#   nlp = spacy.load('ja_ginza')\n",
        "# doc = nlp('山田太郎は東京都渋谷区に住んでいます。木村 花子は山口に住んでいましたが、現在は引っ越して桐生に住んでいます。山口は近々、仕事の都合で津に転居するという話です。あ、三重県の津市のことです。')\n",
        "\n",
        "# print(\"go!\")\n",
        "\n",
        "# for ent in doc.ents:\n",
        "# \tif ent.label_ == 'Person':\n",
        "# \t\tprint('人名:', ent.text)\n",
        "# \telif ent.label_ == 'City':\n",
        "# \t\tprint('住所:', ent.text)\n"
      ],
      "metadata": {
        "id": "Nw1GSvfwk_6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U2P48VE-lKtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib"
      ],
      "metadata": {
        "id": "dYHBXnlek6jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd = os.getcwd()\n",
        "pwd"
      ],
      "metadata": {
        "id": "yC--lOuAlfg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_files(file1_path, file2_path):\n",
        "    with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
        "        diff = difflib.ndiff(file1.readlines(), file2.readlines())\n",
        "        print(''.join(diff))"
      ],
      "metadata": {
        "id": "hyPnv4zglrVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file1_path = './入力フォルダのパス/20240420_1546中央郵便.txt'\n",
        "# file2_path = './出力フォルダのパス/20240420_1546中央郵便.txt'\n",
        "# compare_files(file1_path, file2_path)"
      ],
      "metadata": {
        "id": "kZaUpMralUMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "\n",
        "# nlp = spacy.load('ja_ginza')\n",
        "# doc = nlp('山田太郎は東京都渋谷区に住んでいます。木村 花子は山口に住んでいましたが、現在は引っ越して桐生に住んでいます。山口は近々、仕事の都合で津に転居するという話です。あ、三重県の津市のことです。')\n",
        "\n",
        "# print(\"go!\")\n",
        "\n",
        "# for ent in doc.ents:\n",
        "# \tif ent.label_ == 'Person':\n",
        "# \t\tprint('人名:', ent.text)\n",
        "# \telif ent.label_ == 'City':\n",
        "# \t\tprint('住所:', ent.text)\n"
      ],
      "metadata": {
        "id": "pxFM9KXx1-IM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}